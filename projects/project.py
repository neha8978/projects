# -*- coding: utf-8 -*-
"""project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19KQIGVYZPF4FqoH-hLBHlnBKKNXV7gKW
"""

import pandas as pd
import numpy as np
import re
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

# Load datasets
apps_df = pd.read_csv("/content/Play Store Data.csv")
reviews_df = pd.read_csv("/content/User Reviews.csv")

# Select required columns
reviews_df = reviews_df[['App', 'Translated_Review', 'Sentiment']]
reviews_df.dropna(inplace=True)  # Drop missing values

# Text Preprocessing Function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Apply text preprocessing
reviews_df['Processed_Review'] = reviews_df['Translated_Review'].apply(clean_text)

# Encode Sentiment Labels
label_encoder = LabelEncoder()
reviews_df['Sentiment_Label'] = label_encoder.fit_transform(reviews_df['Sentiment'])

# Prepare text data for deep learning
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(reviews_df['Processed_Review'])
sequences = tokenizer.texts_to_sequences(reviews_df['Processed_Review'])
padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')

# Prepare target labels
Y = to_categorical(reviews_df['Sentiment_Label'])

# Split into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(padded_sequences, Y, test_size=0.2, random_state=42)

# Define LSTM model
model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=100),
    LSTM(64, return_sequences=True),
    Dropout(0.3),
    LSTM(32),
    Dropout(0.3),
    Dense(3, activation='softmax')  # 3 output classes (Positive, Neutral, Negative)
])

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, Y_train, epochs=2, batch_size=32, validation_data=(X_test, Y_test))

# Evaluate model
loss, accuracy = model.evaluate(X_test, Y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

import pandas as pd
import numpy as np
import re
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Load datasets
apps_df = pd.read_csv("/content/Play Store Data.csv")
reviews_df = pd.read_csv("/content/User Reviews.csv")

# Select required columns
reviews_df = reviews_df[['App', 'Translated_Review', 'Sentiment']]
reviews_df.dropna(inplace=True)  # Drop missing values

# Text Preprocessing Function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Apply text preprocessing
reviews_df['Processed_Review'] = reviews_df['Translated_Review'].apply(clean_text)

# Encode Sentiment Labels
label_encoder = LabelEncoder()
reviews_df['Sentiment_Label'] = label_encoder.fit_transform(reviews_df['Sentiment'])  # 0: Negative, 1: Neutral, 2: Positive

# Split dataset
X_train_text, X_test_text, Y_train, Y_test = train_test_split(reviews_df['Processed_Review'],
                                                              reviews_df['Sentiment_Label'],
                                                              test_size=0.2, random_state=42)

### PART 1: MACHINE LEARNING MODELS ###
# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train_text)
X_test_tfidf = vectorizer.transform(X_test_text)

# Train ML models
models = {
    "Logistic Regression": LogisticRegression(max_iter=500),
    "Random Forest": RandomForestClassifier(n_estimators=100),
    "SVM": SVC(kernel='linear')
}

results = {}
conf_matrices = {}

for name, model in models.items():
    model.fit(X_train_tfidf, Y_train)
    y_pred = model.predict(X_test_tfidf)

    accuracy = accuracy_score(Y_test, y_pred)
    precision = precision_score(Y_test, y_pred, average='weighted')
    recall = recall_score(Y_test, y_pred, average='weighted')
    f1 = f1_score(Y_test, y_pred, average='weighted')

    results[name] = {"Accuracy": accuracy, "Precision": precision, "Recall": recall, "F1-Score": f1}

    # Store confusion matrix
    conf_matrices[name] = confusion_matrix(Y_test, y_pred)

### PART 2: LSTM DEEP LEARNING MODEL ###
# Tokenization & Padding
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train_text)
X_train_seq = tokenizer.texts_to_sequences(X_train_text)
X_test_seq = tokenizer.texts_to_sequences(X_test_text)
X_train_pad = pad_sequences(X_train_seq, maxlen=100, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=100, padding='post', truncating='post')

# Convert labels to categorical format
Y_train_cat = to_categorical(Y_train)
Y_test_cat = to_categorical(Y_test)

# Define LSTM Model
lstm_model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=100),
    LSTM(64, return_sequences=True),
    Dropout(0.3),
    LSTM(32),
    Dropout(0.3),
    Dense(3, activation='softmax')  # 3 output classes (Positive, Neutral, Negative)
])

# Compile the model
lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
lstm_model.fit(X_train_pad, Y_train_cat, epochs=10, batch_size=32, validation_data=(X_test_pad, Y_test_cat), verbose=1)

# Evaluate LSTM Model
y_pred_lstm = np.argmax(lstm_model.predict(X_test_pad), axis=1)

accuracy_lstm = accuracy_score(Y_test, y_pred_lstm)
precision_lstm = precision_score(Y_test, y_pred_lstm, average='weighted')
recall_lstm = recall_score(Y_test, y_pred_lstm, average='weighted')
f1_lstm = f1_score(Y_test, y_pred_lstm, average='weighted')

results["LSTM Deep Learning"] = {
    "Accuracy": accuracy_lstm,
    "Precision": precision_lstm,
    "Recall": recall_lstm,
    "F1-Score": f1_lstm
}

# Store confusion matrix for LSTM
conf_matrices["LSTM Deep Learning"] = confusion_matrix(Y_test, y_pred_lstm)

# Display Comparative Results
print("\nModel Comparison Results:")
for model, metrics in results.items():
    print(f"\n{model}:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")

### PLOTTING RESULTS ###

# Convert results dictionary to DataFrame for plotting
results_df = pd.DataFrame(results).T

# Bar Plot for Metrics Comparison
plt.figure(figsize=(12, 6))
results_df.plot(kind="bar", figsize=(12, 6))
plt.title("Model Performance Comparison")
plt.xlabel("Model")
plt.ylabel("Score")
plt.xticks(rotation=45)
plt.legend(loc='lower right')
plt.grid(axis='y')
plt.show()

# Plot Confusion Matrices
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

for ax, (model_name, conf_matrix) in zip(axes.flatten(), conf_matrices.items()):
    sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, ax=ax)
    ax.set_title(f"Confusion Matrix - {model_name}")
    ax.set_xlabel("Predicted")
    ax.set_ylabel("Actual")

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import re
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Load datasets

# Load datasets
apps_df = pd.read_csv("/content/Play Store Data.csv")
reviews_df = pd.read_csv("/content/User Reviews.csv")
# Select required columns
reviews_df = reviews_df[['App', 'Translated_Review', 'Sentiment']]
reviews_df.dropna(inplace=True)

# Text Preprocessing Function
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Apply text preprocessing
reviews_df['Processed_Review'] = reviews_df['Translated_Review'].apply(clean_text)

# Encode Sentiment Labels
label_encoder = LabelEncoder()
reviews_df['Sentiment_Label'] = label_encoder.fit_transform(reviews_df['Sentiment'])  # 0: Negative, 1: Neutral, 2: Positive

# Split dataset
X_train_text, X_test_text, Y_train, Y_test = train_test_split(reviews_df['Processed_Review'],
                                                              reviews_df['Sentiment_Label'],
                                                              test_size=0.2, random_state=42)

# Tokenization & Padding
tokenizer = Tokenizer(num_words=15000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train_text)
X_train_seq = tokenizer.texts_to_sequences(X_train_text)
X_test_seq = tokenizer.texts_to_sequences(X_test_text)
X_train_pad = pad_sequences(X_train_seq, maxlen=150, padding='post', truncating='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=150, padding='post', truncating='post')

# Convert labels to categorical format
Y_train_cat = to_categorical(Y_train, num_classes=3)
Y_test_cat = to_categorical(Y_test, num_classes=3)

# Define Improved LSTM Model
lstm_model = Sequential([
    Embedding(input_dim=15000, output_dim=200, input_length=150),
    Bidirectional(LSTM(128, return_sequences=True)),
    GlobalMaxPooling1D(),  # Helps reduce overfitting
    Dense(64, activation='relu'),
    Dropout(0.4),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(3, activation='softmax')  # 3 output classes (Positive, Neutral, Negative)
])

# Compile the model with a lower learning rate
optimizer = Adam(learning_rate=0.0005)
lstm_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

# Early Stopping and ReduceLROnPlateau Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)

# Train the model
history = lstm_model.fit(X_train_pad, Y_train_cat, epochs=20, batch_size=64,
                         validation_data=(X_test_pad, Y_test_cat), verbose=1,
                         callbacks=[early_stopping, reduce_lr])

# Evaluate LSTM Model
y_pred_lstm = np.argmax(lstm_model.predict(X_test_pad), axis=1)

# Calculate performance metrics
accuracy_lstm = accuracy_score(Y_test, y_pred_lstm)
precision_lstm = precision_score(Y_test, y_pred_lstm, average='weighted')
recall_lstm = recall_score(Y_test, y_pred_lstm, average='weighted')
f1_lstm = f1_score(Y_test, y_pred_lstm, average='weighted')

# Store confusion matrix
conf_matrix_lstm = confusion_matrix(Y_test, y_pred_lstm)

# Print performance metrics
print("\nImproved LSTM Model Performance:")
print(f"Accuracy: {accuracy_lstm:.4f}")
print(f"Precision: {precision_lstm:.4f}")
print(f"Recall: {recall_lstm:.4f}")
print(f"F1-Score: {f1_lstm:.4f}")

# Plot Training & Validation Loss
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training & Validation Loss Curve')
plt.show()

# Plot Accuracy
plt.figure(figsize=(8, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training & Validation Accuracy Curve')
plt.show()

# Confusion Matrix Plot
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix_lstm, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Improved LSTM Model")
plt.show()

